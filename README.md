# Gutenberg Book Crawler

This project provides a command-line tool to download books from Project Gutenberg and store them locally.

## How to run

### Prereqs
- Python 3.9+ on Windows
- Optional: a virtual environment

### Setup (PowerShell)
```powershell
cd source_code_root_dir

# (optional) create and activate a venv
python -m venv .venv
.venv\Scripts\Activate.ps1
```

### Run the pipeline (recommended)
```powershell
python main.py 
```

### OPTIONAL: Arguments

- --datalake (default: "datalake")
  - Root folder of the datalake (expects subfolders like YYYYMMDD/HH with header/body files).
  - Change this if your input data lives elsewhere.

- --catalog (default: "metadata/catalog.json")
  - Output path for the metadata catalog JSON produced by the parser (book_id -> {title, author, release_date, language}).
  - Use an absolute path if you want the catalog outside the repo.

- --progress-parser (default: "metadata/progress_parser.json")
  - Progress file for the metadata parser to resume from the last processed day/hour/book.
  - Delete this file to force a full re-parse.

- --db (default: "datamart/datamart.db")
  - SQLite database file path used by the datamart (books table).
  - Created by the initializer if it does not exist.

- --index-output (default: "index/inverted_index.json")
  - Output path for the inverted index JSON (word -> [book_id, ...]) generated by the indexer.

- --progress-indexer (default: "indexer/progress.json")
  - Progress file for the inverted indexer to resume work safely.
  - Delete to rebuild the index from scratch.

- --progress-crawler (default: "crawler/progress.json")
  - Progress file for the crawler/downloader component to resume batches across runs.

- --batch-size (default: 10)
  - Determines how many books will be downloaded each cycle.

- --sleep-seconds (default: 120)
  - Determines how may seconds between each cycle.
